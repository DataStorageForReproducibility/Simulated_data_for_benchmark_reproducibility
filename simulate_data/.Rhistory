global_imputation[3]
global_imputation[1]
head(global_imputation[1])
global_imputation[1][1:3,1:3]
global_imputation[1][1:3,]
global_imputation[1][,1:3]
global_imputation[1]
dim(global_imputation[1])
f.vec
length(f.vec)
cl
alpha_inferred_final
dim(alpha_inferred_final)
length(alpha_inferred_final)
dim(full.data1)
dim(full.data3)
dim(X_all)
X_all[1:3,1:3]
hist(X_all[1,])
hist(X_all[2,])
hist(X_all[3,])
hist(X_all[4,])
hist(X_all[5,])
library(foreach)
x <- foreach(i=1:3) %do% sqrt(i)
x
x
x <- foreach(i=1:3, .combine='c') %do% sqrt(i)
x
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)
foreach(i=1:3) %dopar% sqrt(i)
library(foreach)
x <- foreach(i=1:3) %do% sqrt(i)
x
foreach(i=1:3) %dopar% sqrt(i)
library(doParallel)
registerDoParallel(cores=2)
foreach(i=1:3) %dopar% sqrt(i)
system.time(foreach(i=1:3) %dopar% sqrt(i))
s <- system.time(foreach(i=1:3) %dopar% sqrt(i))
s
s <- system.time(foreach(i=1:3) %do% sqrt(i))
s
s <- foreach(i=1:3) %do% sqrt(i))
s
s <- foreach(i=1:3) %do% sqrt(i)
s
s1 <- system.time(foreach(i=1:3) %do% sqrt(i))
s1$elapsed
s1
source('/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/SCRIPTS_R/foreach_doParallel.R', echo=TRUE)
s1
typeof(s1)
s1[1]
s1[2]
s1[3]
s2 <- system.time(foreach(i=1:3) %dopar% sqrt(i))
s2[3]
s1 <- system.time(foreach(i=1:3) %do% sqrt(i))
s1[3]
s2 <- system.time(foreach(i=1:3) %dopar% sqrt(i))
s2[3]
s1 <- system.time(foreach(i=1:3) %do% sqrt(i))
s1[3]
s2 <- system.time(foreach(i=1:3) %dopar% sqrt(i))
s2[3]
s2 <- system.time(foreach(i=1:3) %dopar% sqrt(i))
s2[3]
s1 <- system.time(foreach(i=1:3) %do% sqrt(i))
s1[3]
getDoParWorkers()
?filtered.data
??
q
??filtered.data
x1=dnorm(x, mean = 0, sd = 1, log = FALSE)
x2=dnorm(x, mean = 2, sd = 1, log = FALSE)
x3=dnorm(x, mean = 3, sd = 1, log = FALSE)
plot(x1*x2*x3)
x1=dnorm(x, mean = 0, sd = 1, log = FALSE)
x1
x1=dnorm(100, mean = 0, sd = 1, log = FALSE)
x2=dnorm(100, mean = 2, sd = 1, log = FALSE)
x3=dnorm(100, mean = 3, sd = 1, log = FALSE)
plot(x1*x2*x3)
x1
require(graphics)
dnorm(0) == 1/sqrt(2*pi)
dnorm(1) == exp(-1/2)/sqrt(2*pi)
dnorm(1) == 1/sqrt(2*pi*exp(1))
## Using "log = TRUE" for an extended range :
par(mfrow = c(2,1))
plot(function(x) dnorm(x, log = TRUE), -60, 50,
main = "log { Normal density }")
library(mixtools)
wait = faithful$waiting
mixmdl = normalmixEM(wait)
plot(mixmdl,which=2)
lines(density(wait), lty=2, lwd=2)
library(mixtools)
wait = faithful$waiting
mixmdl = normalmixEM(wait)
plot(mixmdl,which=2)
lines(density(wait), lty=2, lwd=2)
mixmdl
library(ADPclust)
dat=read.csv("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/Analysis/MATLAB/MAP-DP_Gaussian_Poisson/manual/600/subset/subset.csv")
View(dat)
dat=read.csv("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/Analysis/MATLAB/MAP-DP_Gaussian_Poisson/manual/600/49_tSNE/tSNE.csv")
View(dat)
library(ADPclust)
dat=read.csv("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/Analysis/MATLAB/MAP-DP_Gaussian_Poisson/manual/600/49_tSNE/tSNE.csv")
run_kmeans<-function(dat, kmax){
ans <- adpclust(dat,htype = "amise", nclust = 2:kmax)
plot(ans)
centers=dat[ans$centers,]
k_means=kmeans(dat, centers, ans$nclust)
return(list(labels=k_means$cluster))
}
run_kmeans(dat, 15)
result=run_kmeans(dat, 15)
table(result)
ans <- adpclust(dat,htype = "amise", nclust = 2:kmax)
plot(ans)
centers=dat[ans$centers,]
k_means=kmeans(dat, centers, ans$nclust)
kmax=15
ans <- adpclust(dat,htype = "amise", nclust = 2:kmax)
ans
ans1 <- adpclust(dat,htype = "amise", nclust = 2:kmax)
ans$clusters
ans1$clusters
ans$clusters
ans1$clusters
ans$nclust
ans1$nclust
ans
ans1
?adpclust
install.packages("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/PACKAGES_zip/autoencoder_1.1.tar.gz", repos = NULL, type = "source")
library(autoencoder)
data('training_matrix_N=5e3_Ninput=100')
View(training.matrix)
dim(training.matrix)
View(training.matrix)
nl=3                          ## number of layers (default is 3: input, hidden, output)
unit.type = "logistic"        ## specify the network unit type, i.e., the unit
nl=3                          ## number of layers (default is 3: input, hidden, output)
unit.type = "logistic"        ## specify the network unit type, i.e., the unit
Nx.patch=10                   ## width of training image patches, in pixels
Ny.patch=10                   ## height of training image patches, in pixels
N.input = Nx.patch*Ny.patch  ## number of units (neurons) in the input layer (one unit per pixel)
N.hidden = 5*5                ## number of units in the hidden layer
lambda = 0.0002               ## weight decay parameter
beta = 6                      ## weight of sparsity penalty term
rho = 0.01                    ## desired sparsity parameter
epsilon <- 0.001              ## a small parameter for initialization of weights
## as small gaussian random numbers sampled from N(0,epsilon^2)
max.iterations = 2000         ## number of iterations in optimizer
autoencoder.object <- autoencode(X.train=training.matrix, nl=nl, N.hidden=N.hidden,
unit.type=unit.type, lambda=lambda, beta=beta,rho=rho,epsilon=epsilon,
optim.method="BFGS",max.iterations=max.iterations,
rescale.flag=TRUE,rescaling.offset=0.001)
visualize.hidden.units(autoencoder.object, Nx.patch, Ny.patch)
X.output <- predict(autoencoder.object, X.input=training.matrix, hidden.output=FALSE)$X.output
autoencoder.object
autoencoder.object$W
dim(autoencoder.object$W)
autoencoder.object$W[1:3,1:3]
autoencoder.object$W
length(autoencoder.object$W)
autoencoder.object$W[1:2,1:2]
autoencoder.object$W[1,1:2]
autoencoder.object$W[1,1]
class(autoencoder.object$W)
autoencoder.object$W[[1]]
autoencoder.object$W[[2]]
autoencoder.object$W[[3]]
visualize.hidden.units
autoencoder.object$nl
names(training.matrix)
colnames(training.matrix)
sce <- SingleCellExperiment(list(counts=training.matrix))
library(scater)
sce <- SingleCellExperiment(list(counts=training.matrix))
View(training.matrix)
runTSNE(sce, ncomponents =2, exprs_values = "counts", perplexity=70, rand_seed = 100)
runTSNE(sce, ncomponents =2, exprs_values = "counts", perplexity=30, rand_seed = 100)
plotTSNE(sce, ncomponents =2, exprs_values = "counts", perplexity=30, rand_seed = 100)
plotTSNE(sce, ncomponents =2, exprs_values = "counts", perplexity=60, rand_seed = 100)
sce <- SingleCellExperiment(list(counts=t(training.matrix)))
plotTSNE(sce, ncomponents =2, exprs_values = "counts", perplexity=60, rand_seed = 100)
sce
View(training.matrix)
plotTSNE(sce, ncomponents =2, exprs_values = "counts", perplexity=60, rand_seed = 100)
plotTSNE(sce, ncomponents =2, exprs_values = "counts", perplexity=30, rand_seed = 100)
sce
View(training.matrix)
?sce <- SingleCellExperiment(list(counts=t(training.matrix)))
plotTSNE
?plotTSNE
plotTSNE(sce, ncomponents =2, exprs_values = "counts", perplexity=30, rand_seed = 100)
Rtsne.default(X = t(training.matrix))
plotTSNE(sce, ncomponents =2, exprs_values = "counts", perplexity=30, rand_seed = 100, check_duplicates = FALSE)
plotTSNE(sce, ncomponents = 2, exprs_values = "counts", perplexity=60, rand_seed = 100, check_duplicates = FALSE)
autoencoder.object
visualize.hidden.units
autoencoder.object$sl
autoencoder.object$W[[1]]
autoencoder.object$W[[2]]
autoencoder.object$W[[3]]
length(autoencoder.object$W[[1]])
autoencoder.object$W[[1]][1:3]
visualize.hidden.units
autoencoder.object$W[[1]][1,]
autoencoder.object$W[[1]][2,]
autoencoder.object$W[[1]][3,]
matrix(autoencoder.object$W[[1]])
class(autoencoder.object$W[[1]])
dim(autoencoder.object$W[[1]])
dim(autoencoder.object$W[[2]])
dim(autoencoder.object$W[[3]])
dim(autoencoder.object$W[[1]])
dim(training.matrix)
visualize.hidden.units
autoencoder.object$sl
autoencoder.object$W[[1]][1,]
autoencoder.object$W[[1]][2,]
autoencoder.object$W[[1]][3,]
dim(autoencoder.object$W[[1]])
visualize.hidden.units
X.output <- predict(autoencoder.object, X.input=training.matrix, hidden.output=FALSE)$X.output
dim(X.output)
op <- par(no.readonly = TRUE)  ## save the whole list of settable par
par(mfrow=c(3,2),mar=c(2,2,2,2))
for (n in c(7,26,16)){
## input image:
image(matrix(training.matrix[n,],nrow=Ny.patch,ncol=Nx.patch),axes=FALSE,main="Input image",
col=gray((0:32)/32))
## output image:
image(matrix(X.output[n,],nrow=Ny.patch,ncol=Nx.patch),axes=FALSE,main="Output image",
col=gray((0:32)/32))
q
q
q
q
op <- par(no.readonly = TRUE)  ## save the whole list of settable par
par(mfrow=c(3,2),mar=c(2,2,2,2))
for (n in c(7,26,16)){
## input image:
image(matrix(training.matrix[n,],nrow=Ny.patch,ncol=Nx.patch),axes=FALSE,main="Input image", col=gray((0:32)/32))
## output image:
image(matrix(X.output[n,],nrow=Ny.patch,ncol=Nx.patch),axes=FALSE,main="Output image", col=gray((0:32)/32))
}
par(op)  ## restore plotting par
q
op <- par(no.readonly = TRUE)  ## save the whole list of settable par
par(mfrow=c(3,2),mar=c(2,2,2,2))
for (n in c(7,26,16)){
## input image:
image(matrix(training.matrix[n,],nrow=Ny.patch,ncol=Nx.patch),axes=FALSE,main="Input image", col=gray((0:32)/32))
## output image:
image(matrix(X.output[n,],nrow=Ny.patch,ncol=Nx.patch),axes=FALSE,main="Output image", col=gray((0:32)/32))
}
par(op)  ## restore plotting par
predict
?predict
X.output
View(X.output)
View(training.matrix)
View(training.matrix)
autoencoder.object$W
W <- autoencoder.object$W
b <- autoencoder.object$b
25.90+19.90
17,90+24,90
17.90+24.90+17.90+16.90
17.90+16.90
install.packages("devtools")
devtools::install_github(repo = "hhoeflin/hdf5r")
devtools::install_github(repo = "mojaveazure/loomR", ref = "develop")
install.packages("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/PACKAGES_zip/hdf5r_1.0.0.tar.gz", repos = NULL, type = "source")
R--version
R.version()
version
library(CountClust)
source("http://bioconductor.org/biocLite.R")
biocLite("CountClust")
version
library(CountClust)
library("devtools")
devtools::install_github("lingxuez/SOUP")
install.packages("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/PACKAGES_zip/PMA_1.0.11.tar.gz", repos = NULL, type = "source")
install.packages("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/PACKAGES_zip/impute_1.26.0.tar.gz", repos = NULL, type = "source")
install.packages("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/PACKAGES_zip/PMA_1.0.11.tar.gz", repos = NULL, type = "source")
library(SOUP)
devtools::install_github("lingxuez/SOUP")
library(SOUP)
library(pcaReduce)
library(devtools)
install_github("wt2015-github/DIMMSC")
install_github( '10XDev/cellrangerRkit', user = 'github_user',
auth_token = 'some_auth_token' )
library(devtools)
install_github("IMB-Computational-Genomics-Lab/ascend")
install.packages("RaceID")
library(RaceID)
library(devtools)
install_github("GIS-SP-Group/RCA")
library(RCA)
library(scran)
library(Seurat)
install.packages('Seurat')
library(Seurat)
source("https://bioconductor.org/biocLite.R")
biocLite("SIMLR")
library(SIMLR)
library(devtools)
devtools::install_github("TSCAN","zji90")
library(TSCAN)
library(devtools)
install_github("wt2015-github/DIMMSC")
library(cellrangerRkit)
source("http://cf.10xgenomics.com/supp/cell-exp/rkit-install-2.0.0.R")
install.packages(package)
library(devtools)
install_github("wt2015-github/DIMMSC")
library(DIMMSC)
devtools::install_github("xu-lab/SINCERA")
library(SINCERA)
install.packages("devtools")
install.packages("devtools")
devtools::install_github("Japrin/sscClust")
library(sscClust)
library(devtools)
install_github("lichen-lab/BAMMSC")
?biocLite
lapply(paste('package:',names(sessionInfo()$otherPkgs),sep=""),detach,character.only=TRUE,unload=TRUE)
load("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/Analysis/UPDATE_R/packs.RData")
View(packs)
.libPaths()
load("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/Analysis/MARIA/Maria_Bisulfite/mathylkit_analysis/Unvsall.RData")
Un=(filtered.UN_All[[1]]$numCs/filtered.UN_All[[1]]$coverage)*100
pG=(filtered.UN_All[[2]]$numCs/filtered.UN_All[[2]]$coverage)*100
pR=(filtered.UN_All[[3]]$numCs/filtered.UN_All[[3]]$coverage)*100
C13=(filtered.UN_All[[4]]$numCs/filtered.UN_All[[4]]$coverage)*100
C50=(filtered.UN_All[[5]]$numCs/filtered.UN_All[[5]]$coverage)*100
C7=(filtered.UN_All[[6]]$numCs/filtered.UN_All[[6]]$coverage)*100
C35=(filtered.UN_All[[7]]$numCs/filtered.UN_All[[7]]$coverage)*100
# t1=(filtered.UN_All[[1]]$numTs/filtered.UN_All[[1]]$coverage)*100
# t2=(filtered.UN_All[[2]]$numTs/filtered.UN_All[[2]]$coverage)*100
# t3=(filtered.UN_All[[3]]$numTs/filtered.UN_All[[3]]$coverage)*100
# t4=(filtered.UN_All[[4]]$numTs/filtered.UN_All[[4]]$coverage)*100
# t5=(filtered.UN_All[[5]]$numTs/filtered.UN_All[[5]]$coverage)*100
# t6=(filtered.UN_All[[6]]$numTs/filtered.UN_All[[6]]$coverage)*100
# t7=(filtered.UN_All[[7]]$numTs/filtered.UN_All[[7]]$coverage)*100
library(ggplot2)
df=data.frame(value=c(Un, pG, pR, C13, C50, C7, C35), sample=c(rep("Un", length(Un)),rep("pG", length(pG)), rep("pR", length(pR)), rep("C13", length(C13)), rep("C50", length(C50)), rep("C7", length(C7)), rep("C35", length(C35)) ))
qplot(aes(sample, value) df, geom="histogram", facet=sample)
qplot(aes(sample, value), df, geom="histogram", facet=sample)
View(df)
hist(Un, freq = FALSE, breaks=10)
hist(pG,  freq = FALSE, breaks=10)
hist(pR,  freq = FALSE, breaks=10)
hist(C13,  freq = FALSE, breaks=10)
par(mfrow=c(3,3))
hist(Un, freq = FALSE, breaks=10)
hist(pG,  freq = FALSE, breaks=10)
hist(pR,  freq = FALSE, breaks=10)
hist(C13,  freq = FALSE, breaks=10)
hist(C50, freq = FALSE,  breaks=10)
hist(C7,  freq = FALSE, breaks=10)
hist(C35, freq = FALSE,  breaks=10)
df=data.frame("x"=c(1,2,3,4), "y"=c(1,2,3,4))
df
plot(x,y)
plot(df$x,df$y)
df2=data.frame("x"=c(1,2,3,4), "y"=c(2,2,3,3))
df2
df2=data.frame("x"=c(1,2,3,4), "y"=c(2,2,3,3))
df2
plot(df1$x,df1$y)
plot(df2$x,df2$y)
df1=data.frame("x"=c(1,2,3,4), "y"=c(1,2,3,4))
df1
df2=data.frame("x1"=c(1,2,3,4), "y1"=c(2,2,3,3))
df2
df1=data.frame("x"=c(1,2,3,4), "y"=c(1,2,3,4))
df1
df2=data.frame("x1"=c(1,2,3,4), "y1"=c(2,2,3,3))
df2
plot(df1$x,df1$y)
plot(df2$x,df2$y)
df1=data.frame("x"=c(1,2,3,4), "y"=c(1,2,3,4))
df1
df2=data.frame("x1"=c(1,2,3,4), "y1"=c(2,2,3,3))
df2
plot(df1$x,df1$y)
plot(df2$x,df2$y)
staccerplot <- function(sth){
plot(sth[1], sth[2])
}
staccerplot <- function(sth){
output=plot(sth[1], sth[2])
return(output)
}
splot <- function(sth){
output=plot(sth[1], sth[2])
return(output)
}
splot(df1)
splot <- function(sth){
plot(sth[1], sth[2])
}
splot(df1)
df1[1]
splot(df1)
splot <- function(sth){
plot(sth)
}
splot(df1)
plot(df1)
plot(df2)
df2
splot <- function(sth){
plot(sth)
}
splot(df1)
splot(df1)
splot(df2)
un=data.frame("x"=c(1,2,3,4), "y"=c(1,2,3,4))
un
pg=data.frame("x1"=c(1,2,3,4), "y1"=c(2,2,3,3))
pg
un=data.frame("x"=c(1,2,3,4), "y"=c(1,2,3,4), "y1"=c(1,2,3,4))
un
pg=data.frame("x1"=c(1,2,3,4), "y1"=c(2,2,3,3), "y2"=c(2,2,3,3))
pg
splot <- function(sth){
plot(sth[1], sth[3])
}
splot(df1)
splot <- function(sth){
plot(sth[,1], sth[,3])
}
splot(df1)
splot(un)
splot <- function(sth1, sth2){
plot(sth1[,1], sth2[,3])
}
splot(un, pg)
getwd()
setwd("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/Analysis/Clustering_Comparison/preprocess_data")
data.path="/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/Analysis/Clustering_Comparison/import_data/Rdata"
data=list.files(data.path)
data
for(i in 1:length(data)){
print(data[i])
load(paste0(data.path,"/", data[i]))
rmarkdown::render("QC_markdown.Rmd", output_file=paste0("Reports/QC_",sub('\\.Rdata$', '', data[i]), ".html"))
}
#Run scripts
main.path="/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/Analysis/Clustering_Comparison/import_data"
setwd(main.path)
bash_scripts=list.files(paste0(main.path, "/bash"))
R_scripts=list.files(paste0(main.path, "/R"))
df=data.frame(bash_scripts,R_scripts)
df
df
#Run scripts
main.path="/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/Analysis/Clustering_Comparison/import_data"
setwd(main.path)
bash_scripts=list.files(paste0(main.path, "/bash"))
R_scripts=list.files(paste0(main.path, "/R"))
df=data.frame(bash_scripts,R_scripts)
df
load("/media/monika/683375f9-7173-4a68-ba60-f33b0c47a470/Analysis/Benchmarking_Clustering_Methods_scRNAseq/import_data/Rdata_import/Baron2016_h.Rdata")
counts(sce)
dim(counts(sce))
dim(counts(sce))[1]
dropout_rate <- function(x){(length(which(x==0))/dim(counts(sce))[1])*100}
apply(counts(data), 2, dropout_rate)[1:7]
apply(counts(sce), 2, dropout_rate)[1:7]
apply(counts(sce)[,1], 2, dropout_rate)[1:7]
dim(counts(sce))[1]
counts(sce)[,1]
apply(counts(sce)[,1], 2, dropout_rate)
dropout_rate <- function(x){(length(which(x==0))/dim(counts(sce))[1])*100}
dropout_rate <- function(x){(length(which(x==0))/dim(x)[1])*100}
apply(counts(sce)[,1], 2, dropout_rate)
dropout_rate <- function(x){(length(which(x==0))/length(x))*100}
apply(counts(sce)[,1], 2, dropout_rate)
length(counts(sce)[,1)
length(counts(sce)[,1])
(length(which(counts(sce)[,1]==0))/length(counts(sce)[,1]))*100
(length(which(counts(sce)[,2]==0))/length(counts(sce)[,2]))*100
(length(which(counts(sce)[,3]==0))/length(counts(sce)[,3]))*100
(length(which(counts(sce)[,4]==0))/length(counts(sce)[,4]))*100
(length(which(counts(sce)[,5]==0))/length(counts(sce)[,5]))*100
(length(which(counts(sce)[,100]==0))/length(counts(sce)[,100]))*100
(length(which(counts(sce)[1,]==0))/length(counts(sce)[,]))*100
(length(which(counts(sce)[1,]==0))/length(counts(sce)[1,]))*100
(length(which(counts(sce)[2,]==0))/length(counts(sce)[2,]))*100
(length(which(counts(sce)[3,]==0))/length(counts(sce)[3,]))*100
(length(which(counts(sce)[1000,]==0))/length(counts(sce)[1000,]))*100
?cluster_similarity
??cluster_similarity
install.packages("clusteval")
#Jaccard
library(clusteval)
